---
title: "Logistic Regression"
author: "Edneide Ramalho"
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output: 
    html_document:
      highlight: textmate
      logo: logo.png
      theme: cerulean
      number_sections: yes
      toc: yes
      toc_float:
        collapsed: yes
        smooth_scroll: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Why you need logistic regression

-   When you have dichotomous variables, you can not use linear regression.

## Churn data

```{r}
#install.packages("devtools")
#devtools::install_github("fstPackage/fst", ref = "develop")
```

```{r}
library(fst) # to read fst format
library(tidyverse)

# data
churn <- read_fst(
  "churn.fst",
  columns = NULL,
  from = 1,
  to = NULL,
  as.data.table = FALSE,
  old_format = FALSE
)
```

## Linear model

```{r}
mdl_churn_vs_recency_lm <- lm(has_churned ~ time_since_last_purchase, data = churn)
mdl_churn_vs_recency_lm
```

```{r}
coeffs <- coefficients(mdl_churn_vs_recency_lm)
intercept <- coeffs[1]
slope <- coeffs[2]
```

## Visualizing the linear model

```{r}
lm_plot <- ggplot(
  churn,
  aes(time_since_last_purchase, has_churned)
) +
  geom_point() +
  geom_abline(intercept = intercept,
              slope = slope)
lm_plot
```

## Zooming out

```{r}
lm_plot +
  xlim(-10, 10) +
  ylim(-0.2, 1.2)
```

## What is logistic regression?

-   Another type of generalized linear model.

-   Used when the response variable is logical.

-   The responses follow logistic (S-shaped) curve.

## Visualizing the logistic model

```{r}
glm_plot <- lm_plot +
  geom_smooth(
    method = "glm",
    se = FALSE,
    method.args = list(family = binomial)
  ) 
glm_plot
```

```{r}
glm_plot +
  xlim(-20, 20) +
  ylim(0, 1)
```

## Exercises

### Exploring the explanatory variables

When the response variable is logical, all the points lie on the y equals zero and y equals one lines, making it difficult to see what is happening. In the video, until you saw the trend line, it wasn't clear how the explanatory variable was distributed on each line. This can be solved with a histogram of the explanatory variable, faceted on the response.

You will use these histograms to get to know the financial services churn dataset seen in the video.

`churn` is available and `ggplot2` is loaded.

-   Using `churn`, plot `time_since_last_purchase` as a histogram with binwidth `0.25` faceted in a grid with `has_churned` on each row.

```{r}
ggplot(data = churn, aes(x = time_since_last_purchase)) +
  geom_histogram(binwidth = 0.25) +
  facet_grid(rows = vars(has_churned))
  
```

-   Redraw the plot with `time_since_first_purchase`. That is, using `churn`, plot `time_since_first_purchase` as a histogram with binwidth `0.25` faceted in a grid with `has_churned` on each row.

```{r}
ggplot(data = churn, aes(x = time_since_first_purchase)) +
  geom_histogram(binwidth = 0.25) +
  facet_grid(rows = vars(has_churned))
```

-   In the `time_since_the_last_purchase` plot, the distribution of churned customers was further right than the distribution of churned customers (churners typically has longer since their last purchase).

-   For `time_since_first_purchase` the opposite is true: churners have a shorter length relationship.

### **Visualizing linear and logistic models**

As with linear regressions, ggplot2 will draw model predictions for a logistic regression without you having to worry about the modeling code yourself. To see how the predictions differ for linear and logistic regressions, try drawing both trend lines side by side. Spoiler: you should see a linear (straight line) trend from the linear model, and a logistic (S-shaped) trend from the logistic model.

`churn` is available and `ggplot2` is loaded.

-   Using `churn` plot `has_churned` vs. `time_since_first_purchase` as a scatter plot, adding a red linear regression trend line (without a standard error ribbon).

```{r}
# Using churn plot has_churned vs. time_since_first_purchase
ggplot(churn, aes(x = time_since_first_purchase, y = has_churned)) +
  # Make it a scatter plot
  geom_point() +
  # Add an lm trend line, no std error ribbon, colored red
  geom_smooth(method = "lm", se = FALSE, color = "red")
```

-   Update the plot by adding a second trend line from logistic regression. (No standard error ribbon again).

```{r}
# Using churn plot has_churned vs. time_since_first_purchase
ggplot(churn, aes(x = time_since_first_purchase, y = has_churned)) +
  # Make it a scatter plot
  geom_point() +
  # Add an lm trend line, no std error ribbon, colored red
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  geom_smooth(method = "glm",
    se = FALSE,
    method.args = list(family = binomial))
```

### **Logistic regression with glm()**

Linear regression and logistic regression are special cases of a broader type of models called *generalized linear models* ("GLMs"). A linear regression makes the assumption that the residuals follow a Gaussian (normal) distribution. By contrast, a logistic regression assumes that residuals follow a binomial distribution.

Here, you'll model how the length of relationship with a customer affects churn.

`churn` is available.

-   Fit a logistic regression of `has_churned` versus `time_since_first_purchase` using the `churn` dataset. Assign to `mdl_churn_vs_relationship`.

```{r}
mdl_churn_vs_relationship <- glm(has_churned ~ time_since_first_purchase, data = churn, family = binomial)
mdl_churn_vs_relationship
```

# Predictions and odds ratio

-   Model:

```{r}
mdl_recency <- glm(
  has_churned ~ time_since_last_purchase,
  data = churn,
  family = "binomial"
)
```

-   Creating explanatory data:

```{r}
explanatory_data <- tibble(
  time_since_last_purchase = seq(-1, 6, 0.25)
)
```

-   Prediction:

```{r}
prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    has_churned = predict(mdl_recency, explanatory_data, type = "response")
  )
```

# Quantifying logistic regression fit

## Adding point predictions

```{r}
plt_churn_vs_recency_base <- ggplot(data = churn, aes(x = time_since_last_purchase, y = has_churned)) + 
  geom_point() +
  geom_smooth(method = "glm",
    se = FALSE,
    method.args = list(family = binomial))
```

```{r}
plt_churn_vs_recency_base +
  geom_point(
    data = prediction_data,
    color = "blue"
  )
```

## Getting the most likely outcome

```{r}
prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    has_churned = predict(mdl_recency, explanatory_data,
                          type = "response"),
    most_likely_outcome = round(has_churned)
  )
```

```{r}
plt_churn_vs_recency_base +
  geom_point(
    aes(y = most_likely_outcome),
    data = prediction_data,
    color = "green"
  )
```

-   For the most recently customers the most likely outcome is that they don't churn. Otherwise, the most likely outcome is that they will churn.

## Odds Ratios

-   *Odds ratio (OR)* é a probabilidade de que algo aconteça, dividido pela probabilidade de não acontecer.

$$
\text{OR} = \dfrac{\text{probabilidade}}{1 - \text{probabilidade}}
$$

## Calculando o *odds ratio*

```{r}
prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    has_churned = predict(mdl_recency, explanatory_data, type = "response"),
    most_likely_response = round(has_churned),
    odds_ratio = has_churned / (1 - has_churned)
  )
```

## Visualizando odds ratio

```{r}
ggplot(
  prediction_data,
  aes(time_since_last_purchase, odds_ratio)
) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dotted")
```

-   A linha pontilhada mostra OR = 1, que significa que a chance de churn é igual a de não-churn.

-   Na parte inferior esquerda, temos que OR \< 1, que significa que a chance de churn é menor do que de não-churn.

-   Na parte superior direita, a chance de churn é cerca de 5 vezes maior que não-churn.

## Exercícios

### **Probabilities**

There are four main ways of expressing the prediction from a logistic regression model -- we'll look at each of them over the next four exercises. Firstly, since the response variable is either "yes" or "no", you can make a prediction of the probability of a "yes". Here, you'll calculate and visualize these probabilities.

Three variables are available:

-   `mdl_churn_vs_relationship` is the logistic regression model of `has_churned` versus `time_since_first_purchase`.

-   `explanatory_data` is a data frame of explanatory values.

-   `plt_churn_vs_relationship` is a scatter plot of `has_churned` versus `time_since_first_purchase` with a smooth glm line.

`dplyr` is loaded.

##### 

-   Use the model, `mdl_churn_vs_relationship`, and the explanatory data, `explanatory_data`, to predict the probability of churning. Assign the predictions to the `has_churned` column of a data frame, `prediction_data`. *Remember to set the prediction* `type`.

```{r}
# Modelo
mdl_churn_vs_relationship <- glm(has_churned ~ time_since_first_purchase, data = churn, family = binomial)

# Dados exploratórios - para fazer predição
explanatory_data <- tibble(
  time_since_first_purchase = c(-1.50, -1.25, -1.00, -0.75, -0.50, -0.25,  0.00,  0.25,  0.50,  0.75,  1.00,  1.25, 1.50,  1.75,  2.00,  2.25,  2.50,  2.75,  3.00,  3.25,  3.50,  3.75,  4.00))

# Base de dados com Predição
prediction_data <- explanatory_data %>% 
  dplyr::mutate(
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"))
```

-   Update the `plt_churn_vs_relationship` plot to add points from `prediction_data`, colored yellow.

```{r}
plt_churn_vs_relationship <- ggplot(data = churn, aes(x = time_since_first_purchase, y = has_churned)) +
  geom_point() +
  geom_smooth(method = "glm", se = FALSE, method.args = list(family = binomial))
plt_churn_vs_relationship
```

```{r}
# Update the plot
plt_churn_vs_relationship +
  # Add points from prediction_data, colored yellow, size 2
  geom_point(
    shape = 21,
    data = prediction_data,
    aes(y = has_churned),
    color = "white",
    fill = "yellow",
    stroke = 1.5
  )
```

### Most likely outcome

When explaining your results to a non-technical audience, you may wish to side-step talking about probabilities and simply explain the most likely outcome. That is, rather than saying there is a 60% chance of a customer churning, you say that the most likely outcome is that the customer will churn. The tradeoff here is easier interpretation at the cost of nuance.

-   Update `prediction_data` to add a column of the most likely churn outcome, `most_likely_outcome`.

```{r}
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"),
    # Add the most likely churn outcome
    most_likely_outcome = round(has_churned)
  )

# See the result
prediction_data
```

-   Update `plt_churn_vs_relationship`, adding yellow points of size 2 with `most_likely_outcome` as the y aesthetic, using `prediction_data`.

```{r}
# Update the plot
plt_churn_vs_relationship +
  # Add most likely outcome points from prediction_data, 
  # colored yellow, size 2
  geom_point(data = prediction_data,
             aes(y=most_likely_outcome),
             color = "yellow", size = 2)
```

### **Odds ratio**

Odds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices. For example, if a customer has a 20% chance of churning, it maybe more intuitive to say "the chance of them not churning is four times higher than the chance of them churning".

-   Update `prediction_data` to add a column, `odds_ratio`, of the odds ratios.

```{r}
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(
      mdl_churn_vs_relationship, explanatory_data, 
      type = "response"
    ),
    # Add the odds ratio
     odds_ratio = has_churned / (1 - has_churned)
  )

# See the result
prediction_data
```

-   Using `prediction_data`, draw a line plot of `odds_ratio` versus `time_since_first_purchase`. Add a dotted horizontal line at `odds_ratio` equal to `1`.

```{r}
ggplot(
  prediction_data,
  aes(time_since_first_purchase, odds_ratio)
) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dotted")
```

# Log odds ratio

One downside to probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. This makes it harder to reason about what happens to the prediction when you make a change to the explanatory variable. The logarithm of the odds ratio (the "log odds ratio") does have a linear relationship between predicted response and explanatory variable. That means that as the explanatory variable changes, you don't see dramatic changes in the response metric - only linear changes.

Since the actual values of log odds ratio are less intuitive than (linear) odds ratio, for visualization purposes it's usually better to plot the odds ratio and apply a log transformation to the y-axis scale.

-   Update `prediction_data` to add the log odds ratio calculated two ways. Calculate it from the `odds_ratio`, then directly using `predict()`.

```{r}
# Update the data frame
prediction_data <- explanatory_data %>% 
  mutate(   
    has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = "response"),
    odds_ratio = has_churned / (1 - has_churned),
    # Add the log odds ratio from odds_ratio
    log_odds_ratio = log(odds_ratio),
    # Add the log odds ratio using predict()
    log_odds_ratio2 = predict(mdl_churn_vs_relationship, explanatory_data)
  )

# See the result
prediction_data
```

-   Update the plot to use a logarithmic y-scale.

```{r}
# Update the plot
ggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dotted") +
  # Use a logarithmic y-scale
  scale_y_log10()
```

# Quantificando o ajuste da regressão logística

Os quatro resultados (matriz de confusão):

|                        |   Realmente Falso   | Realmente Verdadeiro |
|:----------------------:|:-------------------:|:--------------------:|
|   **Predito Falso**    |    CORRETO! - TN    | Falso Negativo - FN  |
| **Predito Verdadeiro** | Falso Positivo - FP |    CORRETO! - TP     |

## Matriz de Confusão

```{r}
mdl_recency <- glm(has_churned ~ time_since_last_purchase,
                   data = churn, family = "binomial")
```

```{r}
actual_response <- churn$has_churned
predicted_response <- round(fitted(mdl_recency))
outcomes <- table(predicted_response, actual_response)
outcomes
```

## Visualizando a matriz de confusão: gráfico mosaico

```{r}
library(ggplot2)
library(yardstick)
```

```{r}
confusion <- conf_mat(outcomes)
```

```{r}
autoplot(confusion) +
  xlab("Truth") +
  ylab("Predicted")
```

## Métricas de Performance

```{r}
summary(confusion, event_level = "second")
```

Vamos olhar para algumas métricas:

## Acurácia

```{r}
summary(confusion, event_level = "second") %>% 
  slice(1)
```

-   Acurácia é a proporção de predições corretas:

$$
\text{acurácia} = \dfrac{TN + TP}{TN + FN + TP + FP}
$$

-   quanto maior a acurácia, melhor.

## Sensibilidade

```{r}
summary(confusion, event_level = "second") %>% 
  slice(3)
```

-   Sensibilidade é a proporção de verdadeiros positivos:

$$
\text{sensibilidade} =  \dfrac{TP}{FN + TP}
$$

## Especificidade

```{r}
summary(confusion, event_level = "second") %>% 
  slice(4)
```

-   É a proporção de verdadeiros negativos

$$
\text{especificidade} = \dfrac{TN}{TN + FP}
$$

-   Quanto maior, melhor.

## Exercícios

### **Calculating the confusion matrix**

A *confusion matrix* (occasionally called a *confusion table*) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.

1.  The customer churned and the model predicted that.

2.  The customer churned but the model didn't predict that.

3.  The customer didn't churn but the model predicted they did.

4.  The customer didn't churn and the model predicted that.

`churn` and `mdl_churn_vs_relationship` are available.

-   Get the actual responses from the `has_churned` column of the dataset. Assign to `actual_response`.

-   Get the "most likely" predicted responses from the model. Assign to `predicted_response`.

-   Create a table of counts from the actual and predicted response vectors. Assign to `outcomes`.

```{r}
# Get the actual responses from the dataset
actual_response <- churn$has_churned

# Get the "most likely" responses from the model
predicted_response <- round(fitted(mdl_churn_vs_relationship))

# Create a table of counts
outcomes <- table(predicted_response, actual_response)

# See the result
outcomes
```

### Measuring logistic model performance

Having the confusion matrix as a table object is OK, but a little hard to program with. By converting this to a `yardstick` confusion matrix object, you get methods for plotting and extracting performance metrics.

The confusion matrix, `outcomes` is available as a table object. `ggplot2` and `yardstick` are loaded, and the `yardstick.event_first` option is set to `FALSE`.

-   Convert `outcomes` to a yardstick confusion matrix. Assign to `confusion`.

-   Automatically plot `confusion`.

-   Get performance metrics from `confusion`, remembering that the positive response is in the second column.

```{r}
# Convert outcomes to a yardstick confusion matrix
confusion <- conf_mat(outcomes)

# Plot the confusion matrix
autoplot(confusion)

# Get performance metrics for the confusion matrix
summary(confusion, event_level = "second")
```

Em termos do nosso problema de churn, temos:

-   **Acurácia:** A proporção de clientes que o modelo previu corretamente se iam ou não entrar em churn.

-   **Sensibilidade:** A proporção de clientes que entraram em churn e o modelo previu corretamente que eles iriam entrar em churn.

-   **Especificidade:** A proporção de clientes que não entraram em churn e o modelo previu corretamente que eles não iriam entrar em churn.
